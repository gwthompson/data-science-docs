## Mini-Batch SGD

## SGD + Momentum

### Why Use
- SGD has difficulty navigating ravines
    - Areas where the surface curves more steeply in one dimension than the other
    - In this scenario is oscillates a lot instead of moving more towards the steeper direction
- Adding momentum helps accelerates SGD in the right direction and reduce oscillations

### Intuition
- Push a ball down a hill, goes faster and faster as it gains momentum

### Defn
- Adds a fraction $\tau$ of the update vector last time step to the current update vector
    - $v_t = \tau v_{t-1} + \eta \nabla_{\theta} J(\theta)$
    - $\theta = \theta - v_t$

## Adam

### Intuition
- Behaves like a ball w heavy friction, which thus prefers flat minima in the error surface
    - If the variance is high it will move more slowly

### Defn
- $\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v_t}} + \epsilon} \hat{m_t}$
    - $\hat{m_t}$ = unbiased estimate of mean of gradient
    - $\hat{v_t}$ = unbiased estimate of variance of gradient
    - $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$
    - $v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$

### Tuning
- The authors propose default values of
    - 0.9 for $\beta_1$
    - 0.999 for $\beta_2$
    - $10^8$ for $\epsilon$
    
### When not to use
- https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/
- https://arxiv.org/abs/1705.08292

## Which Optimizer to use
- Input data is sparse => adaptive lr method
- If you want faster convergene adaptive lrs are more likely to get you there and faster
- Adam is the best choice for adaptive lr
- TODO: look into more

## Resources:
- http://ruder.io/optimizing-gradient-descent/
- http://www.deeplearningbook.org/contents/optimization.html